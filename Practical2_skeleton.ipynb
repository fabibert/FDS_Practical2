{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2 : Generative and Discriminative Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will compare the Naïve Bayes Classifier (NBC) and Logistic Regression on several\n",
    "datasets. As part of the practical you should read briefly the following paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Discriminative vs. Generative classifiers: A comparison of logistic regression\n",
    "and naive Bayes**  \n",
    "*Andrew Y. Ng and Michael I. Jordan*  \n",
    "Advances in Neural Information Processing Systems (NIPS) 2001.\n",
    "\n",
    "The paper is available on OLAT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the Introduction and the Experiments sections. The goal of this practical is\n",
    "to qualitatively reproduce some of the experimental results in this paper. You are strongly\n",
    "encouraged to read the rest of the paper, which is rather short and straightforward to read,\n",
    "though some of you may want to skip the formal proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classifier (NBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a Naïve Bayes Classifier from scartch using NumPy. To keep your code tidy,\n",
    "we recommend implementing it as a class. \n",
    "The classifier should be able to handle binary and continuous features. \n",
    "To earn the bonus points, the classifier should be able to handle categorical features as well. \n",
    "Suppose the data has 3\n",
    "different features, the first being binary, the second being continuous and the third being categorical. Write an implementation that you can initialise as follows:\n",
    "\n",
    "    nbc = NBC(feature_types=['b', 'r', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the lines of classifiers provided in sklearn, you want to implement two more functions,\n",
    "**fit** and **predict**. \n",
    "Recall the joint distribution of a generative model: $p(\\mathbf{x}, y \\mid \\theta, \\pi) = p(y \\mid \\pi) \\cdot p(\\mathbf{x} \\mid y, \\theta)$.\n",
    "The **fit** function is to estimate all the parameters ($\\theta$ and $\\pi$) of the NBC, i.e., train the classifier. The **predict** function computes the probabilities that the new input belongs to all classes and\n",
    "then returns the class that has the largest probability, i.e., makes a prediction.\n",
    "\n",
    "    nbc.fit(X_train, y_train)\n",
    "    ypredicted = nbc.predict(X_test)\n",
    "    test_accuracy = np.mean(ypredicted == ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10., 10.)\n",
    "\n",
    "import pickle as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-conditional distributions\n",
    "\n",
    "Before implementing NBC, we first implement the class-conditional distributions $p(\\mathbf{x} \\mid y, \\theta)$. Your implementation should have two functions: **estimate** and **get_log_probability**. \n",
    "\n",
    "- The **estimate** function takes data as input and models the data using some distribution $p(x \\mid \\theta)$, where $\\theta$ is the parameters of this distribution. The function estimates the parameters $\\theta$ using maximum likelihood estimators (MLE). \n",
    "For example, in the case of continuous features, we use the Gaussian distribution to model the data. The estimate function will find the parameters $\\mu$ and $\\sigma$ for the Gaussian distribution with respect to the input data. \n",
    "\n",
    "- The **get_log_probability** function takes as input a new data point $x_{new}$ and returns the log of $p(x_{new} \\mid \\theta)$. For the Gaussian distribution, the function get_probability will return $\\mathcal{N}(x_{new} \\mid \\mu, \\sigma^2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different types of features, you need to use different distributions.\n",
    "You can import statistic libraries (e.g., `scipy.stats`) for the implementation of the distributions. \n",
    "\n",
    "- For **continuous features**: Use Gaussian distribution\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n",
    "- For **binary features**: Use Bernoulli distribution \n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html\n",
    "- For **categorical features**: Use Multinoulli distribution (The multinoulli distribution is a special case of the multinomial distribution, where the number of trials is 1)\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html\n",
    "\n",
    "Example: https://stackoverflow.com/questions/12412895/how-to-calculate-probability-in-a-normal-distribution-given-mean-standard-devi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Issues:**\n",
    "- The probabilities can be very small. To avoid underflow issues, you should compute the log of the probabilities. Read more: (Mur) Chapter 3.5.3 / Lecture Notes\n",
    "- The variance for Gaussian distributions should never be exactly 0, so in\n",
    "case your calculated variance is 0, you may want to set it to a small value such as 1e − 6. This is to ensure that your code never encounters division by zero or\n",
    "taking logarithms of 0 errors. \n",
    "For this practical, please set the small value to 1e-6.\n",
    "- Laplace/Additive smoothing: You want to ensure that the estimates for the parameter for the Bernoulli and Multinoulli random variables is never exactly 0 or 1. For this reason you should consider using Laplace smoothing (https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "For this practical, please set alpha to 1.\n",
    "- For simplicity, you can assume the data values for binary features are integers from {0,1} and the data for a categorical feature with M categories are integers from {0, ..., M-1}.\n",
    "- Fell free to add auxiliary functions or change the parameters of the functions. If you change the parameters of the functions, make sure you change the tests accordingly, so we can test your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.4456745546617293\n"
     ]
    }
   ],
   "source": [
    "#these next three cells are to test the fitting and can be ignored\n",
    "X = np.array([2.70508547,2.10499698,1.76019132,3.42016431,3.47037973,3.67435061,1.84749286,4.3388506,2.27818252,4.65165335])\n",
    "rv = stats.norm()\n",
    "mu, sigma = stats.norm.fit(X)\n",
    "\n",
    "print(stats.norm.logpdf(2, mu, sigma))\n",
    "#print(stats.norm(params).logpdf(4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "mu, variance = stats.norm.fit(X)\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "plt.bar(X, 1, color ='maroon',\n",
    "        width = 0.01)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-9e29344a558a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_log_probability\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "print(params.get_log_probability(np.array([1,2,3,4])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean, variance = params\n",
    "print(stats.norm(mean, variance).pdf(2))\n",
    "print(stats.lognorm(mean, variance).pdf(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0 # for additive smoothing\n",
    "import scipy.stats as stats\n",
    "# Distribution for continuous features\n",
    "class ContFeatureParam:\n",
    "    #as attributes put mean and variance\n",
    "    mean = 0\n",
    "    variance = 0\n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Gaussian distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        self.mean, self.variance = stats.norm.fit(X)\n",
    "        #fix small parameter issues\n",
    "        #TODO!!!!!\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self, X_new):\n",
    "\n",
    "        return stats.norm.pdf(X_new, self.mean, self.variance)\n",
    "\n",
    "\n",
    "    def get_log_probability(self, X_new):\n",
    "        # TODO: return the log of the density values for the input values X_new\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "\n",
    "        return stats.norm.logpdf(X_new, self.mean, self.variance)\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "# Distribution for binary features\n",
    "class BinFeatureParam:\n",
    "    p=0\n",
    "    q=0\n",
    "    ALPHA = 1\n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Bernoulli distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        # X is a vector with binary values\n",
    "        #calculate bernoulli parameters\n",
    "        count_ones = 0\n",
    "        for i in X:\n",
    "            if i == 1:\n",
    "                count_ones += 1\n",
    "        #fix small parameter issues\n",
    "        self.p  = ((count_ones + self.ALPHA)/(len(X) + self.ALPHA *2))\n",
    "        #!!!unsure yet if we can subtract from 1 or need to do additive smoothing as well!!!\n",
    "        self.q = 1-self.p\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self, X_new):\n",
    "        probabilities = []\n",
    "        for i in X_new:\n",
    "            probabilities.append(stats.bernoulli.pmf(i, self.p, loc=0))\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    def get_log_probability(self, X_new):\n",
    "        # TODO: return the log of the probability values for the input values X_new\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        probabilities = []\n",
    "        for i in X_new:\n",
    "            probabilities.append(float(stats.bernoulli.logpmf(i, self.p, loc=0)))\n",
    "        return probabilities\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "# Distribution for categorical features\n",
    "class CatFeatureParam:\n",
    "    p = []\n",
    "    ALPHA = 1\n",
    "    # we need to know the number of categories for the categorical feature\n",
    "    def __init__(self, num_of_categories):\n",
    "        self._num_of_categories = num_of_categories\n",
    "        self.p = []\n",
    "    \n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Multinoulli distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        d = self._num_of_categories\n",
    "        for number in range(0,self._num_of_categories):\n",
    "            count = 0\n",
    "            for i in X:\n",
    "                if i == number:\n",
    "                    count += 1\n",
    "            #fix small parameter issues (vgl Aufgabenstellung): probability would be count/len(X) but apply additive smoothing\n",
    "            self.p.append(float((count + self.ALPHA)/(len(X) + self.ALPHA *d)))\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self,X_new):\n",
    "        probabilities = []\n",
    "        for i in X_new:\n",
    "            #number i corresponds to the category which we got one from\n",
    "            sample_vec = [0] * self._num_of_categories #10 categories in total\n",
    "            sample_vec[i] = 1 #categories vector with one at given input number\n",
    "            prob = float(stats.multinomial.pmf(sample_vec, 1,self.p)) #category vector probability drawn from model\n",
    "            probabilities.append(prob)\n",
    "        return probabilities\n",
    "        \n",
    "    def get_log_probability(self, X_new):\n",
    "        # TODO: return the log of the probability values for the input values X_new\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        #return stats.multinomial.logpmf(X_new, sum(X_new),self.p)\n",
    "\n",
    "\n",
    "        probabilities = []\n",
    "\n",
    "        for i in X_new:\n",
    "            #number i corresponds to the category which we got one from\n",
    "            sample_vec = [0] * self._num_of_categories #10 categories in total\n",
    "            sample_vec[i] = 1 #categories vector with one at given input number\n",
    "            prob = float(stats.multinomial.logpmf(sample_vec, 1,self.p)) #category vector prpbability drawn from model\n",
    "            probabilities.append(prob)\n",
    "            sample_vec = []\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests:**\n",
    "    \n",
    "We will use the code below to test the correctness of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ContFeatureParam object at 0x7f94493000b8>\n",
      "None\n",
      "[0.04843274 0.23558711 0.40616623 0.24819667]\n",
      "[-3.02757918 -1.44567455 -0.90099277 -1.39353383]\n"
     ]
    }
   ],
   "source": [
    "# continuous features\n",
    "X = np.array([2.70508547,2.10499698,1.76019132,3.42016431,3.47037973,3.67435061,1.84749286,4.3388506,2.27818252,4.65165335])\n",
    "\n",
    "param = ContFeatureParam()\n",
    "print(param)\n",
    "output = param.estimate(X)\n",
    "print(output)\n",
    "probs1 = param.get_probability(np.array([1,2,3,4]))\n",
    "print(probs1)\n",
    "\n",
    "probs2 = param.get_log_probability(np.array([1,2,3,4]))\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41666666666666663, 0.5833333333333334]\n",
      "[-0.8754687373539001, -0.5389965007326869]\n"
     ]
    }
   ],
   "source": [
    "# binary features\n",
    "X = np.array([0,0,1,1,0,1,0,1,1,1])\n",
    "\n",
    "param = BinFeatureParam()\n",
    "param.estimate(X)\n",
    "probs1 = param.get_probability(np.array([0,1]))\n",
    "print(probs1)\n",
    "\n",
    "probs2 = param.get_log_probability(np.array([0,1]))\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14285714285714285, 0.11428571428571427, 0.14285714285714285, 0.08571428571428572, 0.11428571428571427, 0.057142857142857134, 0.14285714285714285, 0.028571428571428577, 0.11428571428571427, 0.057142857142857155]\n",
      "[-1.9459101490553135, -2.169053700369523, -1.9459101490553135, -2.456735772821304, -2.169053700369523, -2.8622008809294686, -1.9459101490553135, -3.5553480614894135, -2.169053700369523, -2.862200880929468]\n"
     ]
    }
   ],
   "source": [
    "# categorical features (bonus task)\n",
    "X = np.array([0,6,5,4,0,6,6,4,1,1,2,3,8,8,1,6,4,9,0,2,2,3,8,0,2])\n",
    "\n",
    "param = CatFeatureParam(num_of_categories=10)\n",
    "param.estimate(X)\n",
    "probs1 = param.get_probability(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "print(probs1)\n",
    "probs2 = param.get_log_probability(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement NBC\n",
    "\n",
    "We are now ready to implement NBC. We follow the structure of models in scikit-learn. We implement NBC as a class with functions **init**, **fit** and **predict**.\n",
    "The **init** function takes as input the types of features and initialise the classifier. The **fit** function takes the training data as input and estimates the parameters. The **predict** function predicts the label for the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Issues:**\n",
    "- You should use matrix operations rather than loops. In general, loops over classes or features are OK, but loops over the rows of data are not a good idea.\n",
    "- The probabilities can be very small. To avoid underflow issues, you should do the calculations in log space. Read more: (Mur) Chapter 3.5.3 / Lecture Note\n",
    "- For simplicity, you can assume the data values for binary features are integers from {0, 1} and the data for a categorical feature with M categories are integers from {0, ..., M-1}.\n",
    "- Fell free to add auxiliary functions or change the parameters of the functions. If you change the parameters of the functions, make sure you change the tests accordingly, so we can test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "# Your task is to implement the three functions of NBC.\n",
    "\n",
    "class NBC:\n",
    "    # Inputs:\n",
    "    #   feature_types: the array of the types of the features, e.g., feature_types=['b', 'r', 'c']\n",
    "    distParamVec = []\n",
    "    distY = 0\n",
    "\n",
    "    condiDistMatrix = []\n",
    "    margProb = 0\n",
    "    y_unique = []\n",
    "    def __init__(self, feature_types):\n",
    "        # TODO: \n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "\n",
    "\n",
    "        for feature in feature_types:\n",
    "            if feature == 'r':\n",
    "                self.distParamVec.append(ContFeatureParam())\n",
    "            if feature == 'b':\n",
    "                self.distPparamVec.append(BinFeatureParam())\n",
    "            if feature == 'c':\n",
    "                self.distParamVec.append(CatFeatureParam(len(feature_types)))\n",
    "            else:\n",
    "                self.distParamVec.append(ContFeatureParam())\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "        \n",
    "    # The function uses the input data to estimate all the parameters of the NBC\n",
    "    def fit(self, X, y):\n",
    "        # TODO: \n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        self.y_unique = set(y) #get all unique category labels\n",
    "\n",
    "        #estimate distribution for targets\n",
    "        self.distY = CatFeatureParam(max(self.y_unique)+1) #instantiate dist with #unique category labels\n",
    "        self.distY.estimate(y)\n",
    "\n",
    "        #for each unique y find all x which have this target -> distribution p(x|y)\n",
    "        #fit this subset of rows to a distribution and get the parameters\n",
    "        self.condiDistMatrix = [[0]*(max(self.y_unique)+1)]*len(X[0]) #for every x we have every y (later used as [i][y]\n",
    "        print(numpy.shape(self.condiDistMatrix))\n",
    "\n",
    "        for n in self.y_unique: #n represents a unique y\n",
    "            print(\"n: %s\" %n)\n",
    "            for i in range(len(X[0])):\n",
    "                print(\"x_i: %s\" %i) #feature number\n",
    "                y_list = numpy.where(y==n) #row indices of a specific y\n",
    "                print(\"list of rows with category %s: %s\" %(n, y_list))\n",
    "                condiData = X[y_list] #get rows which are labeled with specific category\n",
    "                holderParamVec = self.distParamVec[i] #get distribution of a specific feature\n",
    "                #print(\"Conditional data: %s\" %condiData)\n",
    "                holderParamVec.estimate(condiData[:][i]) #get column for estimation of the feature distribution\n",
    "                self.condiDistMatrix[i][n] = holderParamVec #save the distribution parameters for feature i under target n\n",
    "\n",
    "\n",
    "        #mean, variance = self.condiDistMatrix[1][1]\n",
    "        #print(\"check parameters: %s, %s\" %(mean, variance))\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "                \n",
    "                \n",
    "    # The function takes the data X as input, and predicts the class for the data\n",
    "    def predict(self, X):\n",
    "        # TODO: \n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        print(\"X: %s\" %X)\n",
    "        print(\"number of features: %s\" %len(X[0])) #number of features is equal to the second dimension of the matrix\n",
    "        print(\"number of categories: %s\" %(max(self.y_unique)+1)) #max label + 1 ~ number of categories\n",
    "        rowPreds = [0]*len(X) # need to predict for #data points = # rows\n",
    "        rowIndex = 0\n",
    "        for row in X: #gives each row predict y\n",
    "            print(\"row %s: %s\" %(rowIndex, X[rowIndex]))\n",
    "            y_probs = [0] * (max(self.y_unique)+1)\n",
    "            for y in self.y_unique: #for every possible label y we see how high the probability is\n",
    "                #logProb(y_c|x_i) = logProb(x_1|y_c) + ... + logProb(x_d|y_c) + logProb(y_c) / (does not matter for max)\n",
    "                for i in range(len(X[0])): #for every feature in a row we add the conditonal probability\n",
    "                    print(\"Conditional of y: %s and feature_value: %s\" %(y, row[i]))\n",
    "                    condiDistHolder = self.condiDistMatrix[i][y] #get conditional distribution parameters of feature i under this y\n",
    "                    logProb = condiDistHolder.get_log_probability(row[i]) #from dist get log probability for a feature value in row i\n",
    "                    print(\"logProb of p(x_i | y == c): %s\" %logProb)\n",
    "                    y_probs[y] = y_probs[y] + logProb\n",
    "                logProbY = self.distY.get_log_probability(np.array([y]))\n",
    "                print(\"logprob y: %s\" %(logProbY))\n",
    "                y_probs[y] = y_probs[y] + logProbY\n",
    "                print(\"logProb of numerator: %s\" %(y_probs[y]))\n",
    "            y_probs = [float(x) for x in y_probs]\n",
    "            print(\"logProbs for every y: %s\" %(y_probs))\n",
    "            y_index = np.argmax(np.array(y_probs))\n",
    "            rowPreds[rowIndex] =  y_index\n",
    "            print(\"chosen index: %s\" %(y_index))\n",
    "            print(\"row Prediction Array: %s\" %(rowPreds))\n",
    "            rowIndex = rowIndex + 1\n",
    "        print(rowPreds)\n",
    "        return rowPreds\n",
    "\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**\n",
    "\n",
    "We will use the code below to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "n: 0\n",
      "x_i: 0\n",
      "list of rows with category 0: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),)\n",
      "x_i: 1\n",
      "list of rows with category 0: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),)\n",
      "x_i: 2\n",
      "list of rows with category 0: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),)\n",
      "x_i: 3\n",
      "list of rows with category 0: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),)\n",
      "n: 1\n",
      "x_i: 0\n",
      "list of rows with category 1: (array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
      "       67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
      "       84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),)\n",
      "x_i: 1\n",
      "list of rows with category 1: (array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
      "       67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
      "       84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),)\n",
      "x_i: 2\n",
      "list of rows with category 1: (array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
      "       67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
      "       84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),)\n",
      "x_i: 3\n",
      "list of rows with category 1: (array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
      "       67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
      "       84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),)\n",
      "n: 2\n",
      "x_i: 0\n",
      "list of rows with category 2: (array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "       113, 114, 115, 116, 117, 118, 119]),)\n",
      "x_i: 1\n",
      "list of rows with category 2: (array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "       113, 114, 115, 116, 117, 118, 119]),)\n",
      "x_i: 2\n",
      "list of rows with category 2: (array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "       113, 114, 115, 116, 117, 118, 119]),)\n",
      "x_i: 3\n",
      "list of rows with category 2: (array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "       113, 114, 115, 116, 117, 118, 119]),)\n",
      "X: [[6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "number of features: 4\n",
      "number of categories: 3\n",
      "row 0: [6.9 3.2 5.7 2.3]\n",
      "Conditional of y: 0 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 0 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.10382491]\n",
      "Conditional of y: 1 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 1 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.10382491]\n",
      "Conditional of y: 2 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 2 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.99112811]\n",
      "logProbs for every y: [-9.103824912385116, -9.103824912385116, -9.991128107386018]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 1: [5.6 2.8 4.9 2. ]\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 0 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.35364388]\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 1 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.35364388]\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 2 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.24094708]\n",
      "logProbs for every y: [-8.353643884144711, -8.353643884144711, -9.240947079145613]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 2: [7.7 2.8 6.7 2. ]\n",
      "Conditional of y: 0 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 0 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.7345273]\n",
      "Conditional of y: 1 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 1 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.7345273]\n",
      "Conditional of y: 2 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 2 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-11.6218305]\n",
      "logProbs for every y: [-10.734527301957892, -10.734527301957892, -11.621830496958793]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 3: [6.3 2.7 4.9 1.8]\n",
      "Conditional of y: 0 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 0 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 0 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88948748]\n",
      "Conditional of y: 1 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 1 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 1 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88948748]\n",
      "Conditional of y: 2 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 2 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 2 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.77679067]\n",
      "logProbs for every y: [-8.889487475745, -8.889487475745, -9.776790670745902]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 4: [6.7 3.3 5.7 2.1]\n",
      "Conditional of y: 0 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 0 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 0 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 0 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.03720652]\n",
      "Conditional of y: 1 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 1 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 1 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 1 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.03720652]\n",
      "Conditional of y: 2 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 2 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 2 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 2 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.92450971]\n",
      "logProbs for every y: [-9.037206519915891, -9.037206519915891, -9.924509714916793]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 5: [7.2 3.2 6.  1.8]\n",
      "Conditional of y: 0 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 0 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.80766271]\n",
      "Conditional of y: 1 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 1 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.80766271]\n",
      "Conditional of y: 2 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 2 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.69496591]\n",
      "logProbs for every y: [-9.807662711081713, -9.807662711081713, -10.694965906082615]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 6: [6.2 2.8 4.8 1.8]\n",
      "Conditional of y: 0 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.7678365]\n",
      "Conditional of y: 1 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.7678365]\n",
      "Conditional of y: 2 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.65513969]\n",
      "logProbs for every y: [-8.767836498192501, -8.767836498192501, -9.655139693193403]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 7: [6.1 3.  4.9 1.8]\n",
      "Conditional of y: 0 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.65777133]\n",
      "Conditional of y: 1 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.65777133]\n",
      "Conditional of y: 2 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 4.9\n",
      "logProb of p(x_i | y == c): -1.6199505445378044\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.54507452]\n",
      "logProbs for every y: [-8.657771328025955, -8.657771328025955, -9.545074523026857]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 8: [6.4 2.8 5.6 2.1]\n",
      "Conditional of y: 0 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.94452006]\n",
      "Conditional of y: 1 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.94452006]\n",
      "Conditional of y: 2 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.83182326]\n",
      "logProbs for every y: [-8.944520060828275, -8.944520060828275, -9.831823255829176]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 9: [7.2 3.  5.8 1.6]\n",
      "Conditional of y: 0 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 0 and feature_value: 1.6\n",
      "logProb of p(x_i | y == c): -2.4801967429447553\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.90903853]\n",
      "Conditional of y: 1 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 1 and feature_value: 1.6\n",
      "logProb of p(x_i | y == c): -2.4801967429447553\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.90903853]\n",
      "Conditional of y: 2 and feature_value: 7.2\n",
      "logProb of p(x_i | y == c): -2.885700001453084\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 2 and feature_value: 1.6\n",
      "logProb of p(x_i | y == c): -2.4801967429447553\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.79634172]\n",
      "logProbs for every y: [-9.909038525708795, -9.909038525708795, -10.796341720709696]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 10: [7.4 2.8 6.1 1.9]\n",
      "Conditional of y: 0 and feature_value: 7.4\n",
      "logProb of p(x_i | y == c): -3.0681764677818313\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 0 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.11179015]\n",
      "Conditional of y: 1 and feature_value: 7.4\n",
      "logProb of p(x_i | y == c): -3.0681764677818313\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 1 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.11179015]\n",
      "Conditional of y: 2 and feature_value: 7.4\n",
      "logProb of p(x_i | y == c): -3.0681764677818313\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 2 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.99909335]\n",
      "logProbs for every y: [-10.111790154962959, -10.111790154962959, -10.99909334996386]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 11: [7.9 3.8 6.4 2. ]\n",
      "Conditional of y: 0 and feature_value: 7.9\n",
      "logProb of p(x_i | y == c): -3.575055540917241\n",
      "Conditional of y: 0 and feature_value: 3.8\n",
      "logProb of p(x_i | y == c): -1.556228603915067\n",
      "Conditional of y: 0 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 0 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.49122535]\n",
      "Conditional of y: 1 and feature_value: 7.9\n",
      "logProb of p(x_i | y == c): -3.575055540917241\n",
      "Conditional of y: 1 and feature_value: 3.8\n",
      "logProb of p(x_i | y == c): -1.556228603915067\n",
      "Conditional of y: 1 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 1 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.49122535]\n",
      "Conditional of y: 2 and feature_value: 7.9\n",
      "logProb of p(x_i | y == c): -3.575055540917241\n",
      "Conditional of y: 2 and feature_value: 3.8\n",
      "logProb of p(x_i | y == c): -1.556228603915067\n",
      "Conditional of y: 2 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 2 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-11.37852854]\n",
      "logProbs for every y: [-10.491225346852895, -10.491225346852895, -11.378528541853797]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 12: [6.4 2.8 5.6 2.2]\n",
      "Conditional of y: 0 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 2.2\n",
      "logProb of p(x_i | y == c): -2.0891757436688683\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88659102]\n",
      "Conditional of y: 1 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 2.2\n",
      "logProb of p(x_i | y == c): -2.0891757436688683\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88659102]\n",
      "Conditional of y: 2 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 2.2\n",
      "logProb of p(x_i | y == c): -2.0891757436688683\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.77389422]\n",
      "logProbs for every y: [-8.886591023898513, -8.886591023898513, -9.773894218899414]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 13: [6.3 2.8 5.1 1.5]\n",
      "Conditional of y: 0 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 0 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 1.5\n",
      "logProb of p(x_i | y == c): -2.555504490953445\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.11541072]\n",
      "Conditional of y: 1 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 1 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 1.5\n",
      "logProb of p(x_i | y == c): -2.555504490953445\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.11541072]\n",
      "Conditional of y: 2 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 2 and feature_value: 2.8\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 1.5\n",
      "logProb of p(x_i | y == c): -2.555504490953445\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.00271391]\n",
      "logProbs for every y: [-9.115410719771068, -9.115410719771068, -10.00271391477197]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 14: [6.1 2.6 5.6 1.4]\n",
      "Conditional of y: 0 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 0 and feature_value: 2.6\n",
      "logProb of p(x_i | y == c): -1.8864241144147047\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 1.4\n",
      "logProb of p(x_i | y == c): -2.6337086908086222\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.33264461]\n",
      "Conditional of y: 1 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 1 and feature_value: 2.6\n",
      "logProb of p(x_i | y == c): -1.8864241144147047\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 1.4\n",
      "logProb of p(x_i | y == c): -2.6337086908086222\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.33264461]\n",
      "Conditional of y: 2 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 2 and feature_value: 2.6\n",
      "logProb of p(x_i | y == c): -1.8864241144147047\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 1.4\n",
      "logProb of p(x_i | y == c): -2.6337086908086222\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.2199478]\n",
      "logProbs for every y: [-9.332644608257672, -9.332644608257672, -10.219947803258574]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 15: [7.7 3.  6.1 2.3]\n",
      "Conditional of y: 0 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.0973079]\n",
      "Conditional of y: 1 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-10.0973079]\n",
      "Conditional of y: 2 and feature_value: 7.7\n",
      "logProb of p(x_i | y == c): -3.3636145561236126\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 6.1\n",
      "logProb of p(x_i | y == c): -2.0891757436688687\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.98461109]\n",
      "logProbs for every y: [-10.097307895730518, -10.097307895730518, -10.98461109073142]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 16: [6.3 3.4 5.6 2.4]\n",
      "Conditional of y: 0 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 0 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.5332239]\n",
      "Conditional of y: 1 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 1 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.5332239]\n",
      "Conditional of y: 2 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 2 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.42052709]\n",
      "logProbs for every y: [-8.53322389862697, -8.53322389862697, -9.420527093627872]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 17: [6.4 3.1 5.5 1.8]\n",
      "Conditional of y: 0 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 0 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 0 and feature_value: 5.5\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.99086329]\n",
      "Conditional of y: 1 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 1 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 1 and feature_value: 5.5\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.99086329]\n",
      "Conditional of y: 2 and feature_value: 6.4\n",
      "logProb of p(x_i | y == c): -2.2716522099976166\n",
      "Conditional of y: 2 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 2 and feature_value: 5.5\n",
      "logProb of p(x_i | y == c): -1.8024270108665514\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.87816649]\n",
      "logProbs for every y: [-8.990863290372081, -8.990863290372081, -9.878166485372983]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 18: [6.  3.  4.8 1.8]\n",
      "Conditional of y: 0 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.58246358]\n",
      "Conditional of y: 1 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.58246358]\n",
      "Conditional of y: 2 and feature_value: 6.0\n",
      "logProb of p(x_i | y == c): -2.034143158585596\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 4.8\n",
      "logProb of p(x_i | y == c): -1.5996753816123879\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.46976678]\n",
      "logProbs for every y: [-8.582463580017267, -8.582463580017267, -9.469766775018169]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 19: [6.9 3.1 5.4 2.1]\n",
      "Conditional of y: 0 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 0 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 0 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 0 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.12410008]\n",
      "Conditional of y: 1 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 1 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 1 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 1 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.12410008]\n",
      "Conditional of y: 2 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 2 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 2 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 2 and feature_value: 2.1\n",
      "logProb of p(x_i | y == c): -2.1471047805986294\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.01140327]\n",
      "logProbs for every y: [-9.124100075310533, -9.124100075310533, -10.011403270311435]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 20: [6.7 3.1 5.6 2.4]\n",
      "Conditional of y: 0 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 0 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 0 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 0 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88369457]\n",
      "Conditional of y: 1 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 1 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 1 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 1 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.88369457]\n",
      "Conditional of y: 2 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 2 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 2 and feature_value: 5.6\n",
      "logProb of p(x_i | y == c): -1.8429773367173845\n",
      "Conditional of y: 2 and feature_value: 2.4\n",
      "logProb of p(x_i | y == c): -1.9820070253488105\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.77099777]\n",
      "logProbs for every y: [-8.883694572052024, -8.883694572052024, -9.770997767052926]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 21: [6.9 3.1 5.1 2.3]\n",
      "Conditional of y: 0 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 0 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 0 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.91555554]\n",
      "Conditional of y: 1 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 1 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 1 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.91555554]\n",
      "Conditional of y: 2 and feature_value: 6.9\n",
      "logProb of p(x_i | y == c): -2.633708690808623\n",
      "Conditional of y: 2 and feature_value: 3.1\n",
      "logProb of p(x_i | y == c): -1.6981547443929812\n",
      "Conditional of y: 2 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.80285874]\n",
      "logProbs for every y: [-8.915555542363395, -8.915555542363395, -9.802858737364296]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 22: [5.8 2.7 5.1 1.9]\n",
      "Conditional of y: 0 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 0 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 0 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.59694584]\n",
      "Conditional of y: 1 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 1 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 1 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.59694584]\n",
      "Conditional of y: 2 and feature_value: 5.8\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 2 and feature_value: 2.7\n",
      "logProb of p(x_i | y == c): -1.842977336717384\n",
      "Conditional of y: 2 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.48424903]\n",
      "logProbs for every y: [-8.596945839249706, -8.596945839249706, -9.484249034250608]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 23: [6.8 3.2 5.9 2.3]\n",
      "Conditional of y: 0 and feature_value: 6.8\n",
      "logProb of p(x_i | y == c): -2.5555044909534455\n",
      "Conditional of y: 0 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.12120362]\n",
      "Conditional of y: 1 and feature_value: 6.8\n",
      "logProb of p(x_i | y == c): -2.5555044909534455\n",
      "Conditional of y: 1 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-9.12120362]\n",
      "Conditional of y: 2 and feature_value: 6.8\n",
      "logProb of p(x_i | y == c): -2.5555044909534455\n",
      "Conditional of y: 2 and feature_value: 3.2\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-10.00850682]\n",
      "logProbs for every y: [-9.121203623464046, -9.121203623464046, -10.008506818464948]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 24: [6.7 3.3 5.7 2.5]\n",
      "Conditional of y: 0 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 0 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 0 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 0 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.82286908]\n",
      "Conditional of y: 1 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 1 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 1 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 1 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.82286908]\n",
      "Conditional of y: 2 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 2 and feature_value: 3.3\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 2 and feature_value: 5.7\n",
      "logProb of p(x_i | y == c): -1.8864241144147051\n",
      "Conditional of y: 2 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.71017228]\n",
      "logProbs for every y: [-8.822869083275775, -8.822869083275775, -9.710172278276676]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 25: [6.7 3.  5.2 2.3]\n",
      "Conditional of y: 0 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.82286908]\n",
      "Conditional of y: 1 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.82286908]\n",
      "Conditional of y: 2 and feature_value: 6.7\n",
      "logProb of p(x_i | y == c): -2.4801967429447562\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.71017228]\n",
      "logProbs for every y: [-8.822869083275776, -8.822869083275776, -9.710172278276678]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 26: [6.3 2.5 5.  1.9]\n",
      "Conditional of y: 0 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 0 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 0 and feature_value: 5.0\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 0 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.93583071]\n",
      "Conditional of y: 1 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 1 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 1 and feature_value: 5.0\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 1 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.93583071]\n",
      "Conditional of y: 2 and feature_value: 6.3\n",
      "logProb of p(x_i | y == c): -2.207930269374879\n",
      "Conditional of y: 2 and feature_value: 2.5\n",
      "logProb of p(x_i | y == c): -1.9327673439585138\n",
      "Conditional of y: 2 and feature_value: 5.0\n",
      "logProb of p(x_i | y == c): -1.6431221593097085\n",
      "Conditional of y: 2 and feature_value: 1.9\n",
      "logProb of p(x_i | y == c): -2.2716522099976157\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.8231339]\n",
      "logProbs for every y: [-8.935830705288808, -8.935830705288808, -9.82313390028971]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 27: [6.5 3.  5.2 2. ]\n",
      "Conditional of y: 0 and feature_value: 6.5\n",
      "logProb of p(x_i | y == c): -2.3382706024668414\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 0 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.85473005]\n",
      "Conditional of y: 1 and feature_value: 6.5\n",
      "logProb of p(x_i | y == c): -2.3382706024668414\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 1 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.85473005]\n",
      "Conditional of y: 2 and feature_value: 6.5\n",
      "logProb of p(x_i | y == c): -2.3382706024668414\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 5.2\n",
      "logProb of p(x_i | y == c): -1.6981547443929816\n",
      "Conditional of y: 2 and feature_value: 2.0\n",
      "logProb of p(x_i | y == c): -2.2079302693748786\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.74203325]\n",
      "logProbs for every y: [-8.854730053587144, -8.854730053587144, -9.742033248588045]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 28: [6.2 3.4 5.4 2.3]\n",
      "Conditional of y: 0 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 0 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 0 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 0 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.44633034]\n",
      "Conditional of y: 1 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 1 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 1 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 1 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.44633034]\n",
      "Conditional of y: 2 and feature_value: 6.2\n",
      "logProb of p(x_i | y == c): -2.14710478059863\n",
      "Conditional of y: 2 and feature_value: 3.4\n",
      "logProb of p(x_i | y == c): -1.619950544537804\n",
      "Conditional of y: 2 and feature_value: 5.4\n",
      "logProb of p(x_i | y == c): -1.7647731368622073\n",
      "Conditional of y: 2 and feature_value: 2.3\n",
      "logProb of p(x_i | y == c): -2.0341431585855956\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.33363354]\n",
      "logProbs for every y: [-8.44633034323233, -8.44633034323233, -9.333633538233231]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "row 29: [5.9 3.  5.1 1.8]\n",
      "Conditional of y: 0 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 0 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 0 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 0 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.59984229]\n",
      "Conditional of y: 1 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 1 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 1 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 1 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-0.8803587226480918]\n",
      "logProb of numerator: [-8.59984229]\n",
      "Conditional of y: 2 and feature_value: 5.9\n",
      "logProb of p(x_i | y == c): -1.9820070253488113\n",
      "Conditional of y: 2 and feature_value: 3.0\n",
      "logProb of p(x_i | y == c): -1.7300157147043502\n",
      "Conditional of y: 2 and feature_value: 5.1\n",
      "logProb of p(x_i | y == c): -1.669190225928101\n",
      "Conditional of y: 2 and feature_value: 1.8\n",
      "logProb of p(x_i | y == c): -2.338270602466841\n",
      "logprob y: [-1.7676619176489943]\n",
      "logProb of numerator: [-9.48714549]\n",
      "logProbs for every y: [-8.599842291096195, -8.599842291096195, -9.487145486097097]\n",
      "chosen index: 0\n",
      "row Prediction Array: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Accuracy: 0.0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# All features of the iris dataset are continuous.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "#print(X)\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc_iris = NBC(feature_types=['r', 'r', 'r', 'r'])\n",
    "nbc_iris.fit(Xtrain, ytrain)\n",
    "yhat = nbc_iris.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 90%\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "print(X)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features of this dataset are binary\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./datasets/binary_test.csv', header=None)\n",
    "data = data.to_numpy()\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc = NBC(feature_types=['b'] * 16)\n",
    "nbc.fit(Xtrain, ytrain)\n",
    "yhat = nbc.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 85%\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features of this dataset are categorical (bonus task)\n",
    "\n",
    "data = pd.read_csv('./datasets/categorical_test.csv', header=None)\n",
    "data = data.to_numpy()\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc = NBC(feature_types=['c'] * 9)\n",
    "nbc.fit(Xtrain, ytrain)\n",
    "yhat = nbc.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 65%\n",
    "print(yhat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression (LR), you should use the implementation in scikit-learn. Add the following\n",
    "line to import the LR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the scikit-learn documentation for the Logistic Regression model:\n",
    "- http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NBC and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "The experiment is to compare the classification error of the NBC and LR trained on increasingly larger training datasets. \n",
    "Since the datasets are so small, you should do this multiple times and\n",
    "average the classification error. One run should look as follows:\n",
    "- Shuffle the data, put 20% aside for testing.\n",
    "    \n",
    "    ```N, D = X.shape\n",
    "    Ntrain = int(0.8 * N)\n",
    "    shuffler = np.random.permutation(N)\n",
    "    Xtrain = X[shuffler[:Ntrain]]\n",
    "    ytrain = y[shuffler[:Ntrain]]\n",
    "    Xtest = X[shuffler[Ntrain:]]\n",
    "    ytest = y[shuffler[Ntrain:]]\n",
    "    \n",
    "    ```  \n",
    "\n",
    "\n",
    "- Train the classifiers with increasingly more data. For example, we can train classifiers with 10%, 20%, ..., 100% of the training data. For each case store the classification errors on the test set of the classifiers.\n",
    "\n",
    "You may want to repeat this with at least 200 random permutations (possibly as large as 1000)\n",
    "to average out the test error across the runs. In the end, you will get average test errors as a function of the size of the training data. \n",
    "We have written for you the function for making the plots for the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs:\n",
    "#   nbc: Naive Bayes Classifier\n",
    "#   lr: Logistic Regression Classifier\n",
    "#   X, y: data\n",
    "#   num_runs: we need repeat num_runs times and store average results\n",
    "#   num_splits: we want to compare the two models on increasingly larger training sets.\n",
    "#               num_splits defines the number of increasing steps. \n",
    "# outputs:\n",
    "#   the arrays of the test errors across the runs of the two classifiers \n",
    "def compareNBCvsLR(nbc, lr, X, y, num_runs=200, num_splits=10):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    tst_errs_nbc = []\n",
    "    tst_errs_lr = []\n",
    "    \n",
    "    return tst_errs_nbc, tst_errs_lr\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlot(tst_errs_nbc, tst_errs_lr, title=None, num_splits=10):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.set_xlabel('Percent of training data used', fontsize=20)\n",
    "    ax.set_ylabel('Classification Error', fontsize=20)\n",
    "    if title is not None: ax.set_title(title, fontsize=25)\n",
    "\n",
    "    xaxis_scale = [(i + 1) * (100/num_splits) for i in range(num_splits)]\n",
    "    plt.plot(xaxis_scale, tst_errs_nbc, label='Naive Bayes')\n",
    "    plt.plot(xaxis_scale, tst_errs_lr, label='Logistic Regression', linestyle='dashed')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks: For each dataset,\n",
    "1. prepare the data for the two classifiers, e.g., handle missing values, handle text/categorical data, etc.\n",
    "2. compare the two classifiers on the dataset and generate the plots\n",
    "3. write a short report of how you prepare the data and your observations of the comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Iris Dataset**\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Voting Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n",
    "\n",
    "The logistic regression line meets the naive bayes line early in the plot. To see it clearer, you should use only 100 data points from the dataset for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "voting = pd.read_csv('./datasets/voting.csv')\n",
    "\n",
    "voting.info()\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
